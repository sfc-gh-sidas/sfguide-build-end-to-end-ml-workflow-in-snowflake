{
 "metadata": {
  "kernelspec": {
   "display_name": "Python37 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "lastEditStatus": {
   "notebookId": "pd2w3itafytfnytotknv",
   "authorId": "158808794318",
   "authorName": "SIKHADAS",
   "authorEmail": "sikha.das@snowflake.com",
   "sessionId": "0984b01c-92b3-4a8b-86cb-984ab3d01e04",
   "lastEditTime": 1748617127821
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e79ae8e5-aec2-4276-9443-074c3a614142",
   "metadata": {
    "collapsed": false,
    "name": "MD_INTRO"
   },
   "source": "# ❄️ Build an End-to-End Distributed Machine Learning Workflow in Snowflake ❄️\n\nIn this Notebook ([on Container Runtime](https://docs.snowflake.com/developer-guide/snowflake-ml/notebooks-on-spcs)), we will develop a machine learning model that accurately predicts the \"mortgage response\" (e.g., loan approval, offer acceptance) based on borrower characteristics and loan details.\n\nWe will showcase all the typical steps in a machine learning pipeline using native capabilities in Snowflake through this use case:\n\n### 1. `FEATURE ENGINEERING:` Use [Snowflake Feature Store](https://docs.snowflake.com/en/developer-guide/snowflake-ml/feature-store/overview) to track engineered features\n- Store feature defintions in a feature store for reproducible computation of ML features\n      \n### 2. `DISTRIBUTED MODEL TRAINING, DEPLOYMENT, & INFERENCE`\n- Distributed XGBoost via [Snowflake's Distributed Modeling Classes](https://docs.snowflake.com/en/developer-guide/snowpark-ml/reference/latest/modeling_distributors) - multi-node, multi-GPU\n- Explore [Snowflake Model Registry](https://docs.snowflake.com/en/developer-guide/snowflake-ml/model-registry/overview) capabilities such as **metadata tracking, model metrics tracking, and inference/deployment**\n- [Deploy model to Snowpark Container Services](https://docs.snowflake.com/en/developer-guide/snowflake-ml/model-registry/container)\n\n### 3. `ML OBSERVABILITY:` Set up [Model Monitors](Model Monitors) to track 1 year of predicted and actual loan repayments\n- Compute performance metrics such a F1, Precision, Recall\n- Inspect model drift (i.e. how much has the average predicted repayment rate changed day-to-day)\n- Compare models side-by-side to understand which model should be used in production\nIdentify and understand data issues"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce110000-1111-2222-3333-ffffff000000",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "imports_and_session",
    "resultHeight": 84
   },
   "outputs": [],
   "source": "# Built-in\nimport math\nimport pickle\nfrom datetime import datetime\n\n# Third-party\nimport pandas as pd\nimport numpy as np\nimport shap\nimport sklearn\nimport streamlit as st\nfrom xgboost import XGBClassifier\n\n# Snowflake ML\nfrom snowflake.ml.registry import Registry\nfrom snowflake.ml.feature_store import FeatureStore, FeatureView, Entity, CreationMode\nfrom snowflake.ml.runtime_cluster import scale_cluster, get_nodes\n\n# Snowpark Core\nfrom snowflake.snowpark import DataFrame, Window\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark.functions import (\n    col,\n    to_timestamp,\n    min,\n    max,\n    month,\n    dayofmonth,\n    dayofweek,\n    dayofyear,\n    avg,\n    date_add,\n    sql_expr,\n)\nfrom snowflake.snowpark.types import IntegerType\n\n# Initialize session\nsession = get_active_session()\n\n# Use schema DEFAULT_SCHEMA to create new Snowflake objects in this project\nsession.use_schema('DEFAULT_SCHEMA')\nsession.use_role('ATTENDEE_ROLE')\n\nsession\n\nimport logging\n#logging.basicConfig(level=logging.WARN)\nlogging.getLogger().setLevel(logging.WARN)\n\n# Let's set a version (must be a string) that will be used through all the artifacts \n# that will be created for our project.\nVERSION_NUM = 'V0'"
  },
  {
   "cell_type": "markdown",
   "id": "c150a19a-9fc5-48ab-8633-4a306ce3097a",
   "metadata": {
    "collapsed": false,
    "name": "md_read_data"
   },
   "source": [
    "We can read in the .csv file and store the data as a Snowflake table. Then, we can read in the data as a [Snowpark dataframe](https://docs.snowflake.com/en/developer-guide/snowpark/python/working-with-dataframes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e853f0a-321f-4bbd-a69d-44b9b6ad8839",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "read_data"
   },
   "outputs": [],
   "source": "try:\n    print(\"Reading table data...\")\n    df = session.table(\"MORTGAGE_LENDING_DEMO_DATA\")\n    df.show(5)\nexcept:\n    print(\"Table not found! Uploading data to snowflake table\")\n    df_pandas = pd.read_csv(\"MORTGAGE_LENDING_DEMO_DATA.csv.zip\")\n    session.write_pandas(df_pandas, \"MORTGAGE_LENDING_DEMO_DATA\", auto_create_table=True)\n    df = session.table(\"MORTGAGE_LENDING_DEMO_DATA\")\n    df.show(5)"
  },
  {
   "cell_type": "markdown",
   "id": "8aa46c7d-519b-422c-8932-9b031fc6b4bd",
   "metadata": {
    "collapsed": false,
    "name": "___FEATURE_ENG___"
   },
   "source": [
    "# Feature Engineering with Snowpark APIs\n",
    "\n",
    "We will create a number of features within `create_mortgage_features()`:\n",
    "- Timestamp features (i.e. month, day of year, day of week)\n",
    "- Income and Loan features\n",
    "- County-level income stats\n",
    "- High income flag\n",
    "- Time-based rolling average.\n",
    "\n",
    "All the features are created using Snowpark DF operations and functions, which you can find the API reference documentation for [here](https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/1.2.0/index). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40b85cc-3bdb-48f9-b562-c67821d65bed",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "create_features"
   },
   "outputs": [],
   "source": [
    "def create_mortgage_features(df):\n",
    "    # Step 1: Timestamp features (Per-row features)\n",
    "    # Get current date and time\n",
    "    current_time = datetime.now()\n",
    "    df_max_time = datetime.strptime(str(df.select(max(\"TS\")).collect()[0][0]), \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    \n",
    "    # Find delta between latest existing timestamp and today's date\n",
    "    timedelta = current_time- df_max_time\n",
    "\n",
    "    df = df.with_columns(\n",
    "        [\"TIMESTAMP\", \"MONTH\", \"DAY_OF_YEAR\", \"DOTW\"],\n",
    "        [\n",
    "            date_add(to_timestamp(\"TS\"), timedelta.days-1),\n",
    "            month(\"TIMESTAMP\"),\n",
    "            dayofyear(\"TIMESTAMP\"),\n",
    "            dayofweek(\"TIMESTAMP\")\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Step 2: Income and loan features (Per-row features)\n",
    "    df = df.with_columns(\n",
    "        [\"LOAN_AMOUNT\", \"INCOME\", \"INCOME_LOAN_RATIO\"],\n",
    "        [\n",
    "            col(\"LOAN_AMOUNT_000s\")*1000,\n",
    "            col(\"APPLICANT_INCOME_000s\")*1000,\n",
    "            col(\"INCOME\")/col(\"LOAN_AMOUNT\")\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Step 3: County-level income stats (Per-group features)\n",
    "    county_income_df = df.group_by([\"COUNTY_NAME\"]).agg(\n",
    "        avg(\"INCOME\").alias(\"AVG_COUNTY_INCOME\")\n",
    "    )\n",
    "    # Join back to the original dataframe\n",
    "    df = df.join(county_income_df, \"COUNTY_NAME\")\n",
    "    \n",
    "    # Step 4: Add high income flag\n",
    "    df = df.with_column(\n",
    "        \"HIGH_INCOME_FLAG\", \n",
    "        (col(\"INCOME\") > col(\"AVG_COUNTY_INCOME\")).astype(IntegerType())\n",
    "    )\n",
    "    \n",
    "    # Step 5: Time-based rolling average\n",
    "    df = df.with_column(\n",
    "        \"AVG_THIRTY_DAY_LOAN_AMOUNT\",\n",
    "        sql_expr(\"\"\"\n",
    "            AVG(LOAN_AMOUNT) OVER (\n",
    "                PARTITION BY COUNTY_NAME \n",
    "                ORDER BY TIMESTAMP \n",
    "                RANGE BETWEEN INTERVAL '30 DAYS' PRECEDING AND CURRENT ROW\n",
    "            )\n",
    "        \"\"\")\n",
    "    )\n",
    "\n",
    "    return df\n",
    "    \n",
    "df = create_mortgage_features(df)\n",
    "\n",
    "feature_df = df.select(\n",
    "        [\"LOAN_ID\", \"TIMESTAMP\", \"MONTH\", \"DAY_OF_YEAR\", \"DOTW\", \n",
    "         \"LOAN_AMOUNT\", \"INCOME\", \"INCOME_LOAN_RATIO\", \n",
    "         \"AVG_COUNTY_INCOME\", \"HIGH_INCOME_FLAG\", \n",
    "         \"AVG_THIRTY_DAY_LOAN_AMOUNT\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62244827-8252-4153-99c4-087798504b6c",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "view_features"
   },
   "outputs": [],
   "source": "# Let's take a look at the features we just created\nfeature_df.limit(5)"
  },
  {
   "cell_type": "markdown",
   "id": "2723dbb3-2519-4c56-8c36-c9294f2a3190",
   "metadata": {
    "collapsed": false,
    "name": "md_df_explain"
   },
   "source": [
    "We can even see what the actual SQL execution plan in under the hood from defining all the feature logic above using Snowpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c4ead8-25ac-46cc-9bd9-17eac2f796d5",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "df_explain",
    "resultHeight": 312
   },
   "outputs": [],
   "source": [
    "feature_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d7645e-e0ac-4539-b132-54ce53431402",
   "metadata": {
    "collapsed": false,
    "name": "md_fs"
   },
   "source": "## Now, we can create a [Snowflake Feature Store](https://docs.snowflake.com/en/developer-guide/snowflake-ml/feature-store/overview).\n\nWe'll go ahead and use our session's current Snowflake database, schema, and warehouse to create it for the purpose of this demo.\n\n### How does the Snowflake Feature Store work?\n\nA feature store contains feature views. \n\nA **feature view** encapsulates a Python or SQL pipeline for transforming raw data into one or more related features. \n\nFeature views are organized in the feature store according to the **entities** to which they apply. An **entity** is a higher-level abstraction that represents the subject matter of a feature. \n\nSo, let's proceed to create a new entity now. Since we are creating features at the loan level, we will create an entity for loans. Entities are used to look up and join features."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abacdc71-9f2c-419f-8d50-3e8f89be367f",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "define_feature_store",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "fs = FeatureStore(\n",
    "    session=session, \n",
    "    database=session.get_current_database(), \n",
    "    name=session.get_current_schema(), \n",
    "    default_warehouse=session.get_current_warehouse(),\n",
    "    creation_mode=CreationMode.CREATE_IF_NOT_EXIST\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91d6d39-7819-4825-8729-a3f19ca5cdf7",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "load_or_register_entity",
    "resultHeight": 38
   },
   "outputs": [],
   "source": [
    "# First try to retrieve an existing entity definition\n",
    "# If not, define a new one and register\n",
    "try:\n",
    "    # Retrieve existing entity\n",
    "    loan_id_entity = fs.get_entity('LOAN_ENTITY') \n",
    "    print('Retrieved existing entity')\n",
    "except:\n",
    "    # Define new entity\n",
    "    loan_id_entity = Entity(\n",
    "        name = \"LOAN_ENTITY\",\n",
    "        join_keys = [\"LOAN_ID\"],\n",
    "        desc = \"Features defined on a per loan level\")\n",
    "    # Register\n",
    "    fs.register_entity(loan_id_entity)\n",
    "    print(\"Registered new entity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67480d6a-183f-4373-aaa8-d3ed8e80e11d",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "list_entities",
    "resultHeight": 111
   },
   "outputs": [],
   "source": [
    "fs.list_entities()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf84fe3-4120-4092-b43d-8873da57d461",
   "metadata": {
    "collapsed": false,
    "name": "md_feature_view"
   },
   "source": [
    "Now, we can create a [Feature View](https://docs.snowflake.com/en/developer-guide/snowflake-ml/feature-store/feature-views) based on the feature engineering logic we created above and applied to the data we read into a Snowpark DF, which is encapusulated into `feature_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b53364f-90c4-45b4-94ee-b2fde6f93475",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "feature_view_creation",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "# Define and register feature view\n",
    "loan_fv = FeatureView(\n",
    "    name=\"Mortgage_Feature_View\",\n",
    "    entities=[loan_id_entity],\n",
    "    feature_df=feature_df,\n",
    "    timestamp_col=\"TIMESTAMP\",\n",
    "    refresh_freq=\"1 day\",\n",
    "    refresh_mode=\"INCREMENTAL\")\n",
    "\n",
    "# Add feature level descriptions\n",
    "loan_fv = loan_fv.attach_feature_desc(\n",
    "    {\n",
    "        \"MONTH\": \"Month of loan\",\n",
    "        \"DAY_OF_YEAR\": \"Day of calendar year of loan\",\n",
    "        \"DOTW\": \"Day of the week of loan\",\n",
    "        \"LOAN_AMOUNT\": \"Loan amount in $USD\",\n",
    "        \"INCOME\": \"Household income in $USD\",\n",
    "        \"INCOME_LOAN_RATIO\": \"Ratio of LOAN_AMOUNT/INCOME\",\n",
    "        \"AVG_COUNTY_INCOME\": \"Average household income aggregated at county level\",\n",
    "        \"HIGH_INCOME_FLAG\": \"Binary flag to indicate whether household income is higher than MEDIAN_COUNTY_INCOME\",\n",
    "        \"AVG_THIRTY_DAY_LOAN_AMOUNT\": \"Rolling 30 day average of LOAN_AMOUNT\"\n",
    "    }\n",
    ")\n",
    "\n",
    "loan_fv = fs.register_feature_view(loan_fv, version=VERSION_NUM, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d2aecb-3593-41ab-8951-3847e707bdaa",
   "metadata": {
    "collapsed": false,
    "name": "md_list_feature_views"
   },
   "source": [
    "Let's take a look at the existing feature views we have in our Feature Store through the Python API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c3225b-b936-4aa7-81f2-27bbaeee1c0f",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "show_feature_views",
    "resultHeight": 111
   },
   "outputs": [],
   "source": [
    "fs.list_feature_views()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85718b0d-55f0-4013-a4fd-57bdee356322",
   "metadata": {
    "collapsed": false,
    "name": "md_inspect_ui"
   },
   "source": [
    "You can also inspect your feature view in the Feature Store UI link generated in the next cell. \n",
    "\n",
    "You can also click on the `Lineage` tab in the Feature View to look at the lineage of these features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7a1aae-0bd2-4aad-b9ed-3347fc56b6ea",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "create_feature_store_link"
   },
   "outputs": [],
   "source": [
    "# Create link to feature store UI to inspect newly created feature view!\n",
    "org_name = session.sql('SELECT CURRENT_ORGANIZATION_NAME()').collect()[0][0]\n",
    "account_name = session.sql('SELECT CURRENT_ACCOUNT_NAME()').collect()[0][0]\n",
    "db_name = session.sql('SELECT CURRENT_DATABASE()').collect()[0][0]\n",
    "schema_name = session.sql('SELECT CURRENT_SCHEMA()').collect()[0][0]\n",
    "\n",
    "st.write(f'https://app.snowflake.com/{org_name}/{account_name}/#/features/database/{db_name}/store/{schema_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96ff67f-bb04-40cb-8c14-11b5ebb2917d",
   "metadata": {
    "collapsed": false,
    "name": "md_dataset"
   },
   "source": [
    "### Retrieve a [Dataset from the feature view for model training](https://docs.snowflake.com/en/developer-guide/snowflake-ml/feature-store/modeling#generating-snowflake-datasets-for-training)\n",
    "\n",
    "We can now generate a [Snowflake Dataset](https://docs.snowflake.com/en/developer-guide/snowflake-ml/dataset), which are immutable, file-based objects that exist within your Snowpark session. \n",
    "\n",
    "They can be written to persistent Snowflake objects as needed.\n",
    "\n",
    "First, we create a \"spine dataframe\" which will contain the rows and entity IDs that we want to include in our training data. In this example, we are selecting all the rows in our source data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1687ccf1-cab1-48b5-a914-f6665cf35a77",
   "metadata": {
    "language": "python",
    "name": "generate_spine_df"
   },
   "outputs": [],
   "source": [
    "# Create a spine dataframe to select the desired rows for training. \n",
    "# In this case we are using all the rows \n",
    "spine_df = df.select(\"LOAN_ID\", \"TIMESTAMP\", \"LOAN_PURPOSE_NAME\",\"MORTGAGERESPONSE\") #only need the features used to fetch rest of feature view\n",
    "spine_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f06788-4a31-4357-83af-e1c1d8bee194",
   "metadata": {
    "collapsed": false,
    "name": "md_generate_dataset"
   },
   "source": [
    "The `generate_dataset()` API will fill this spine_df with the correct feature values from the Feature View."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535efc80-e4fc-41c5-98eb-5b5450bcf199",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "generate_dataset",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "ds = fs.generate_dataset(\n",
    "    name=f\"MORTGAGE_DATASET_EXTENDED_FEATURES_{VERSION_NUM}\",\n",
    "    # only need the features used to fetch rest of feature view\n",
    "    spine_df=df.select(\"LOAN_ID\", \"TIMESTAMP\", \"LOAN_PURPOSE_NAME\",\"MORTGAGERESPONSE\"), \n",
    "    features=[loan_fv],\n",
    "    spine_timestamp_col=\"TIMESTAMP\",\n",
    "    spine_label_cols=[\"MORTGAGERESPONSE\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdaa537-3fb9-476c-9153-3236edfdfcb3",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "convert_dataset_to_snowpark_and_pandas",
    "resultHeight": 239
   },
   "outputs": [],
   "source": [
    "ds_sp = ds.read.to_snowpark_dataframe()\n",
    "ds_sp.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68269d55-c869-41a2-9329-8c47178bba17",
   "metadata": {
    "collapsed": false,
    "name": "md_ohe"
   },
   "source": [
    "Next, we will use [Snowflake ML distributed preprocessors such as OneHotEncoder](https://docs.snowflake.com/en/developer-guide/snowflake-ml/modeling#preprocessing) which are implementations of sklearn preprocessors and run in parallel on the Warehouse. Alternatively you can use OSS sklearn preprocessors directly on the Container Runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e17036-7a69-4915-b025-49c900aeb46b",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "one_hot_encoding",
    "resultHeight": 360
   },
   "outputs": [],
   "source": "import snowflake.ml.modeling.preprocessing as snowml\nfrom snowflake.snowpark.types import StringType\n\nOHE_COLS = ds_sp.select([col.name for col in ds_sp.schema if col.datatype==StringType()]).columns\nOHE_POST_COLS = [i+\"_OHE\" for i in OHE_COLS]\n\n# Encode categoricals to numeric columns\nsnowml_ohe = snowml.OneHotEncoder(input_cols=OHE_COLS, output_cols = OHE_COLS, drop_input_cols=True)\nds_sp_ohe = snowml_ohe.fit(ds_sp).transform(ds_sp)\n\n# Rename columns to avoid double nested quotes and white space chars\nrename_dict = {}\nfor i in ds_sp_ohe.columns:\n    if '\"' in i:\n        rename_dict[i] = i.replace('\"','').replace(' ', '_')\n\nds_sp_ohe = ds_sp_ohe.rename(rename_dict)\nds_sp_ohe.columns"
  },
  {
   "cell_type": "markdown",
   "id": "fcb14f23-2537-4c0f-85e8-c97368879d9e",
   "metadata": {
    "collapsed": false,
    "name": "md_split_sets"
   },
   "source": "Now, we're ready to split the processed data into train/test sets and fill any null values."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d834f6f3-ce15-405e-8fec-1d1bb5c224a6",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "train_test_split",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "train, test = ds_sp_ohe.random_split(weights=[0.70, 0.30], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ff103e-5314-4e95-87ba-d784b1102f36",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "fill_na",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "cols = list(ds_sp_ohe.columns)\n",
    "cols.remove(\"TIMESTAMP\")\n",
    "\n",
    "train = train.fillna(0, subset=cols)\n",
    "test = test.fillna(0, subset=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f44c10-7b1d-48f1-95f4-ffadb23b0a4e",
   "metadata": {
    "collapsed": false,
    "name": "___TRAIN_DEPLOY___"
   },
   "source": "# Distributed model training & deployment\n\nNow we demonstrate how to execute distributed model training using Snowflake's [distributed modeling classes](https://docs.snowflake.com/en/developer-guide/snowpark-ml/reference/latest/modeling_distributors).\n\nSnowflake will set up a ray cluster on all available nodes in your compute pool (CPU or GPU) and execute the distributed training job"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bb1b94-1cfd-4c5e-8a62-3e3b4b1b9c07",
   "metadata": {
    "codeCollapsed": true,
    "language": "python",
    "name": "_ray_resource",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "import pprint\n",
    "\n",
    "def _format_resources(resources):\n",
    "    \"\"\"Convert memory fields to GB and filter out internal node tags.\"\"\"\n",
    "    formatted = {}\n",
    "    for k, v in resources.items():\n",
    "        # Skip internal node identifiers\n",
    "        if k.startswith(\"node:\"):\n",
    "            continue\n",
    "        if k in {\"memory\", \"object_store_memory\"}:\n",
    "            formatted[k] = f\"{v / (1024 ** 3):.2f} GB\"\n",
    "        else:\n",
    "            formatted[k] = v\n",
    "    return formatted\n",
    "\n",
    "def show_ray_cluster_resources():\n",
    "    \"\"\"Nicely formatted cluster-wide and node-level resource info from Ray.\"\"\"\n",
    "    print(\" Cluster Resources:\")\n",
    "    cluster = _format_resources(ray.cluster_resources())\n",
    "    pprint.pprint(cluster, sort_dicts=True, width=100)\n",
    "\n",
    "    print(\"\\n Node-Level Resources:\")\n",
    "    for node in ray.nodes():\n",
    "        print(f\"\\nNode: {node['NodeManagerAddress']}\")\n",
    "        node_resources = _format_resources(node[\"Resources\"])\n",
    "        pprint.pprint(node_resources, sort_dicts=True, width=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbf3e6b-cf17-446b-866d-f98dabc49c1b",
   "metadata": {
    "collapsed": false,
    "name": "md_scale_up"
   },
   "source": [
    "Let's scale our resources to use all 4 nodes in our cluster using Snowflake ML's `scale_cluster` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31149b3-eaa1-4828-9096-e755fff4b932",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "scale_up_cluster_pretrain"
   },
   "outputs": [],
   "source": [
    "# Scale to max compute pool nodes\n",
    "num_nodes = 4\n",
    "\n",
    "# Suppress SIGTERM ray warnings \n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# Use full path name\n",
    "nb_name = \"E2E_ML_NOTEBOOK_DIST\"\n",
    "scale_cluster(expected_cluster_size=num_nodes, \n",
    "              notebook_name=nb_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cfc050-c294-4843-ba42-d3e784ce1644",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "confirm_scale_up"
   },
   "outputs": [],
   "source": [
    "# Show number of nodes after changing cluster manager settings\n",
    "show_ray_cluster_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74fdb36-84bd-46fb-a9c0-5599b7ae61cb",
   "metadata": {
    "collapsed": false,
    "name": "md_ready_to_train"
   },
   "source": [
    "Now, we're ready to do multi-node, multi-GPU training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c59691b-d866-452b-955b-1ef03ea42df6",
   "metadata": {
    "codeCollapsed": true,
    "language": "python",
    "name": "quiet_ray_pretrain",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Quiet messages from Ray\n",
    "context = ray.data.DataContext().get_current() \n",
    "context.execution_options.verbose_progress = False\n",
    "# context.enable_operator_progress_bars = False\n",
    "# context.enable_progress_bars = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44106da8-2fff-4494-94d4-7122f3007895",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "multi_node_multi_gpu"
   },
   "outputs": [],
   "source": [
    "from snowflake.ml.modeling.distributors.xgboost.xgboost_estimator import XGBEstimator, XGBScalingConfig\n",
    "from snowflake.ml.data.data_connector import DataConnector\n",
    "\n",
    "# Use Snowflake's DataConnector to efficiently connect Snowflake data to Ray\n",
    "dc = DataConnector.from_dataframe(train)\n",
    "\n",
    "# Set up the scaling configuration for multi-node and multi-GPU usage\n",
    "scaling_config = XGBScalingConfig(\n",
    "    num_workers=-1,            # Use all available workers\n",
    "    num_cpu_per_worker=-1,     # Use all available CPU cores per worker\n",
    "    use_gpu=True               # Enable GPU for training, resources_per_worker={\"GPU\": 1}\n",
    ")\n",
    "\n",
    "# Define XGBoost hyperparameters for GPU\n",
    "hyp_params = {\n",
    "    \"booster\": \"gbtree\",               # Standard boosting model\n",
    "    \"tree_method\": \"gpu_hist\",         # Enables NV_GPU Histogram\n",
    "    \"predictor\": \"auto\",               # Uses GPU for prediction\n",
    "    \"n_estimators\": 100,               # Number of trees to train\n",
    "    \"max_depth\": 0,                    # Maximum tree depth\n",
    "    \"grow_policy\": \"lossguide\",        # Enables better GPU scalability by growing trees leaf-wise (instead of depth-wise)\n",
    "    \"max_leaves\": 512,                 # Limits tree complexity; balances model capacity and memory usage when using 'lossguide'\n",
    "    \"learning_rate\": 0.1,              # Step size for each tree\n",
    "    \"subsample\": 0.8,                  # Fraction of samples used for each tree\n",
    "    \"colsample_bytree\": 0.8            # Fraction of features used for each tree\n",
    "}\n",
    "\n",
    "# Define distributed xgb estimator\n",
    "multi_node_gpu_xgb = XGBEstimator(\n",
    "    params = hyp_params,\n",
    "    scaling_config = scaling_config)\n",
    "\n",
    "multi_node_gpu_xgb.fit(dc,\n",
    "                 input_cols = train.drop([\"TIMESTAMP\", \"LOAN_ID\", \"MORTGAGERESPONSE\"]).columns,\n",
    "                 label_col = \"MORTGAGERESPONSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ef9ff0-e9ab-40ca-97bc-b59e7450c369",
   "metadata": {
    "collapsed": false,
    "name": "md_dist_model"
   },
   "source": [
    "### Now, we can log this distributed xgb model to Snowflake Model Registry:\n",
    "- Log models with important metadata\n",
    "- Manage model lifecycles\n",
    "- Serve models from Snowflake runtimes"
   ]
  },
  {
   "cell_type": "code",
   "id": "fbf0526c-34e1-4723-a173-60116bd0e8d6",
   "metadata": {
    "language": "python",
    "name": "model_reg"
   },
   "outputs": [],
   "source": "#Create a snowflake model registry object \nfrom snowflake.ml.registry import Registry\nfrom snowflake.ml._internal.utils import identifier\nfrom snowflake.ml.model import model_signature\n\ndb = identifier._get_unescaped_name(session.get_current_database())\nschema = identifier._get_unescaped_name(session.get_current_schema())\n\n# Define model name\nmodel_name = f\"MORTGAGE_LENDING_MLOPS_{VERSION_NUM}\"\n\n# Create a registry to log the model to\nmodel_registry = Registry(session=session, \n                          database_name=db, \n                          schema_name=schema,\n                          options={\"enable_monitoring\": True})",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479a3b58-42e4-4883-aac2-d680b5830a16",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "log_multi_node_xgb",
    "collapsed": false
   },
   "outputs": [],
   "source": "logging.getLogger().setLevel(logging.WARN)\n\n# Extract xgb booster object from Snowflake optimized XGB model\ngpu_booster = multi_node_gpu_xgb.get_booster()\n\n# Log the distributed model to the model registry\ndist_version_name = f'XGB_MULTI_NODE_GPU_DIST'\n\ntry:\n    mv_base = model_registry.get_model(model_name).version(dist_version_name)\n    print(\"Found existing model version!\")\nexcept:\n    print(\"Logging new model version...\")\n    mv_base = model_registry.log_model(\n        model_name=model_name,\n        model=gpu_booster,\n        version_name=dist_version_name,\n        sample_input_data = train.drop([\"TIMESTAMP\", \"LOAN_ID\", \"MORTGAGERESPONSE\"]).limit(100), #using snowpark df to maintain lineage\n        comment = \"\"\"Distributed ML model for predicting loan approval likelihood.\"\"\",\n        options={'relax_version': True},\n        target_platforms={\"WAREHOUSE\", \"SNOWPARK_CONTAINER_SERVICES\"}\n    )"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a208883-0e48-4685-ac30-0e23c3cbfab6",
   "metadata": {
    "language": "sql",
    "name": "show_versions",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": [
    "SHOW VERSIONS IN MODEL {{model_name}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df060c3-a45b-4999-b229-18cfdeec492c",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "show_models_2"
   },
   "outputs": [],
   "source": [
    "model_registry.show_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3934427e-390d-4f5d-b341-756afa53f8c2",
   "metadata": {
    "collapsed": false,
    "name": "md_scale_down"
   },
   "source": [
    "Since we don't need all nodes anymore, let's scale down our cluster now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1f3de4-685f-402f-8caa-427cdb56938b",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "scale_down_post_train"
   },
   "outputs": [],
   "source": [
    "# Scale down \n",
    "num_nodes = 1\n",
    "\n",
    "# Suppress SIGTERM ray warnings \n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# Use full path\n",
    "nb_name = \"E2E_ML_NOTEBOOK_DIST\"\n",
    "scale_cluster(expected_cluster_size=num_nodes, \n",
    "              notebook_name=nb_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c96e53-480c-4df4-8375-e0ad5faec722",
   "metadata": {
    "collapsed": false,
    "name": "md_confirm_scale_down"
   },
   "source": [
    "We can see that 3 of the nodes are empty now, which tells us they're no longer being considered as available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fee6a9-9265-4e2d-848d-f209ba15c891",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "confirm_scale_down"
   },
   "outputs": [],
   "source": [
    "# Show number of nodes after changing cluster manager settings\n",
    "show_ray_cluster_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a8deb6-fdc0-464d-9ad2-b41650a2217a",
   "metadata": {
    "collapsed": false,
    "name": "model_deploy_spcs"
   },
   "source": [
    "## Model Deployment to Snowpark Container Services (SPCS)\n",
    "\n",
    "Here, we also show how to deploy a model to Snowpark Container Services as a long running service using [Model Serving](https://docs.snowflake.com/en/developer-guide/snowflake-ml/model-registry/container).\n",
    "\n",
    "This is useful to run GPU based inference, run very large models that do not fit in the Warehouse memory, or when you have additional package dependencies that are not met in the Warehouse. You can also create REST API endpoints for running inference using external HTTP requests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15be76d-6afe-4020-950c-0e8cc3955a66",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "define_deploy_vars"
   },
   "outputs": [],
   "source": "# Define the variables we'll use to create the deployment service.\nimage_repo_name = \"my_inference_images\"\ncp_name = \"CP_GPU_NV_S_4\"\nnum_spcs_nodes = '2'\nservice_name = 'MORTGAGE_LENDING_PREDICTION_SERVICE'\n\ncurrent_database = session.get_current_database().replace('\"', '')\ncurrent_schema = session.get_current_schema().replace('\"', '')\nextended_image_repo_name = f\"{current_database}.DEFAULT_SCHEMA.{image_repo_name}\"\nextended_service_name = f'{current_database}.DEFAULT_SCHEMA.{service_name}'"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf88db88-6d30-4ca5-aedb-700ab5609c8a",
   "metadata": {
    "language": "sql",
    "name": "drop_if_needed",
    "collapsed": true,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": [
    "DROP SERVICE IF EXISTS {{service_name}};"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca8088c-7a3e-4d65-b669-dae3eafc907d",
   "metadata": {
    "collapsed": false,
    "name": "md_create_service"
   },
   "source": [
    "Note, we're creating a service based on the model we just logged to the Model Registry: `mv_base`\n",
    "\n",
    "You can also specify pip_requirements if your model has pip dependencies. Here we are selecting `ingress_enabled = True` to also create a REST API endpoint for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96365088-1841-4505-88aa-c4bd472a63ce",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "create_service"
   },
   "outputs": [],
   "source": [
    "mv_base.create_service(\n",
    "    service_name=extended_service_name,\n",
    "    service_compute_pool=cp_name,\n",
    "    image_repo=extended_image_repo_name,\n",
    "    ingress_enabled=True,\n",
    "    max_instances=int(num_spcs_nodes),\n",
    "    build_external_access_integration=\"ALLOW_ALL_INTEGRATION\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04424bbb-33af-4c91-a3d7-e6294c9b6d78",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "show_services"
   },
   "outputs": [],
   "source": "-- Check that the service is created\nSHOW SERVICES;"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc147c6-e180-49f4-a863-f7cfe19a5668",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "get_model_services"
   },
   "outputs": [],
   "source": "mv_base = model_registry.get_model(f\"MORTGAGE_LENDING_MLOPS_{VERSION_NUM}\").version(\"XGB_MULTI_NODE_GPU_DIST\")\nmv_base.list_services()"
  },
  {
   "cell_type": "markdown",
   "id": "0ceac92c-b931-4513-ac69-0ba029ed743d",
   "metadata": {
    "collapsed": false,
    "name": "md_run_inference"
   },
   "source": [
    "Now, we can run predictions on test data using this deployed SPCS Model Service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbf6cd1-67e1-4638-95f8-dae3fe69dd4a",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "run_inference"
   },
   "outputs": [],
   "source": [
    "mv_base.run(test, \n",
    "            function_name = \"PREDICT\", \n",
    "            service_name = \"DEFAULT_SCHEMA.MORTGAGE_LENDING_PREDICTION_SERVICE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8209d9-a272-4252-ad0a-bf497031f363",
   "metadata": {
    "name": "__MODEL_OBS__",
    "collapsed": false
   },
   "source": "# Model Observability (Model Monitoring)\n[ML Observability](https://docs.snowflake.com/en/developer-guide/snowflake-ml/model-registry/model-observability) allows you to track the quality of production models you have deployed via the Snowflake Model Registry across multiple dimensions, such as performance, drift, and volume.\n\nLet's first save our train and test data as Snowflake tables and create a new stage for the next few steps."
  },
  {
   "cell_type": "code",
   "id": "4b67dfcd-3778-4133-be27-06bb9523e7f0",
   "metadata": {
    "language": "python",
    "name": "save_train_test"
   },
   "outputs": [],
   "source": "train.write.save_as_table(f\"DEMO_MORTGAGE_LENDING_TRAIN_{VERSION_NUM}\", mode=\"overwrite\")\ntest.write.save_as_table(f\"DEMO_MORTGAGE_LENDING_TEST_{VERSION_NUM}\", mode=\"overwrite\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5b108ce5-5e7b-426a-ab64-578569b6a3e7",
   "metadata": {
    "language": "python",
    "name": "create_stage"
   },
   "outputs": [],
   "source": "session.sql(\"CREATE STAGE IF NOT EXISTS ML_STAGE\").collect()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "33474d3e-12a8-4b55-8c6a-d8c669bfe81b",
   "metadata": {
    "name": "md_inference_sproc",
    "collapsed": false
   },
   "source": "We can create stored procedures for running model inference. These procedures can be scheduled or orchestrated to run continuous inference."
  },
  {
   "cell_type": "code",
   "id": "867671fe-fe59-455b-9f75-a2662232eb53",
   "metadata": {
    "language": "python",
    "name": "inference_sproc"
   },
   "outputs": [],
   "source": "from snowflake import snowpark\nfrom snowflake.ml.registry import Registry\nimport joblib\nimport os\nimport logging\nfrom snowflake.ml.modeling.pipeline import Pipeline\nimport snowflake.ml.modeling.preprocessing as pp\nfrom snowflake.snowpark.types import StringType, IntegerType\nimport snowflake.snowpark.functions as F\n\ndef demo_inference_sproc(session: snowpark.Session, table_name: str, modelname: str, modelversion: str) -> str:\n    database=session.get_current_database()\n    schema=session.get_current_schema()\n    reg = Registry(session=session)\n    m = reg.get_model(model_name)  # Fetch the model using the registry\n    mv = m.version(modelversion)\n    \n    input_table_name=table_name\n    pred_col = f'{modelversion}_PREDICTION'\n\n    # Read the input table to a dataframe\n    df = session.table(input_table_name)\n    # 'results' is the output DataFrame with predictions\n    results = mv.run(df, \n                     function_name=\"predict\",\n                     service_name=\"DEFAULT_SCHEMA.MORTGAGE_LENDING_PREDICTION_SERVICE\").select(\"LOAN_ID\",'\"output_feature_0\"').withColumnRenamed('\"output_feature_0\"', pred_col)\n\n    final = df.join(results, on=\"LOAN_ID\", how=\"full\")\n    final = final.with_column(pred_col,\n                              F.round(col(pred_col)))\n    \n    # Write results back to Snowflake table\n    final.write.save_as_table(table_name, mode='overwrite', enable_schema_evolution=True)\n\n    return \"Success\"\n\n# Register the stored procedure\nsession.sproc.register(\n    func=demo_inference_sproc,\n    name=\"model_inference_sproc\",\n    replace=True,\n    is_permanent=True,\n    stage_location=\"@ML_STAGE\",\n    packages=['joblib', 'snowflake-snowpark-python', 'snowflake-ml-python'],\n    return_type=StringType()\n)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "04bd1388-fdd0-46c8-b2b2-efb711deaab1",
   "metadata": {
    "name": "md_call_sproc",
    "collapsed": false
   },
   "source": "We'll call our stored procedure 2 times which will execute inference using the deployed model on the train and test data."
  },
  {
   "cell_type": "code",
   "id": "c2033373-60a5-4f5c-88dc-d8d2d72a436b",
   "metadata": {
    "language": "sql",
    "name": "call_sproc_train"
   },
   "outputs": [],
   "source": "CALL model_inference_sproc(\n    'DEMO_MORTGAGE_LENDING_TRAIN_{{VERSION_NUM}}',\n    '{{model_name}}', \n    '{{dist_version_name}}');",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6c420eee-fb1b-4fd2-ad52-30dded9b060e",
   "metadata": {
    "language": "sql",
    "name": "call_sproc_test",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "CALL model_inference_sproc(\n    'DEMO_MORTGAGE_LENDING_TEST_{{VERSION_NUM}}',\n    '{{model_name}}', \n    '{{dist_version_name}}');",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5aacc1fc-2193-4ce0-9aab-9b9401fcb224",
   "metadata": {
    "language": "sql",
    "name": "view_preds",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "SELECT * FROM DEMO_MORTGAGE_LENDING_TEST_{{VERSION_NUM}} LIMIT 10",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2b9905ac-a793-4d93-ac45-ed912a70b44b",
   "metadata": {
    "name": "md_model_monitor",
    "collapsed": false
   },
   "source": "### Create Model Monitor\n\nWe'll now create a model monitor for the model and compute the prediction drift.\n\n"
  },
  {
   "cell_type": "code",
   "id": "2db322e2-091f-495c-9602-e320d22fc0b4",
   "metadata": {
    "language": "sql",
    "name": "create_monitor"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE MODEL MONITOR MORTGAGE_LENDING_DIST_MODEL_MONITOR\nWITH\n    MODEL={{model_name}}\n    VERSION={{dist_version_name}}\n    FUNCTION=predict\n    SOURCE=DEMO_MORTGAGE_LENDING_TEST_{{VERSION_NUM}}\n    BASELINE=DEMO_MORTGAGE_LENDING_TRAIN_{{VERSION_NUM}}\n    TIMESTAMP_COLUMN='TIMESTAMP'\n    PREDICTION_SCORE_COLUMNS=('XGB_MULTI_NODE_GPU_DIST_PREDICTION')  \n    ACTUAL_SCORE_COLUMNS=('MORTGAGERESPONSE')\n    ID_COLUMNS=('LOAN_ID')\n    WAREHOUSE={{session.get_current_warehouse()}}\n    REFRESH_INTERVAL='1 min'\n    AGGREGATION_WINDOW='1 day';",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "db14e765-e195-4598-8a5a-9e933760a300",
   "metadata": {
    "language": "sql",
    "name": "model_drift"
   },
   "outputs": [],
   "source": "SELECT * FROM TABLE(MODEL_MONITOR_DRIFT_METRIC(\n    'MORTGAGE_LENDING_DIST_MODEL_MONITOR', 'DIFFERENCE_OF_MEANS', \n    'XGB_MULTI_NODE_GPU_DIST_PREDICTION', \n    '1 DAY', \n    TO_TIMESTAMP_TZ('2024-06-01'), \n    TO_TIMESTAMP_TZ('2024-09-01')))",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "01dd35ac-cbc5-41c0-95ee-caa59aa19894",
   "metadata": {
    "collapsed": false,
    "name": "md_drop_service"
   },
   "source": [
    "Since we created a REST API above, this service will run continuously. It is a good idea to drop or suspend the service if you do not need it. Compute pool will automatically suspend if no service is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952e6063-6d6e-44d3-9b68-e87ca9d5134e",
   "metadata": {
    "language": "sql",
    "name": "drop_service"
   },
   "outputs": [],
   "source": "DROP SERVICE IF EXISTS {{service_name}};\nDROP MODEL IF EXISTS {{model_name}};"
  }
 ]
}