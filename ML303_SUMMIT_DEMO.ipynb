{
 "metadata": {
  "kernelspec": {
   "display_name": "Python37 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "lastEditStatus": {
   "notebookId": "4cshpg67nkqduu2wo637",
   "authorId": "1469951504878",
   "authorName": "CGOYETTE",
   "authorEmail": "colin.goyette@snowflake.com",
   "sessionId": "aedef646-d528-4776-9119-e537c8ddc77a",
   "lastEditTime": 1748972068012
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e79ae8e5-aec2-4276-9443-074c3a614142",
   "metadata": {
    "collapsed": false,
    "name": "MD_INTRO"
   },
   "source": [
    "# ❄️ Build an End-to-End Distributed Machine Learning Workflow in Snowflake ❄️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "BEST_PRACTICE_GIT"
   },
   "source": [
    "# Best Practice: Integrate your git repositories and use a container runtime python environment. \n",
    "On the right hand side, you'll notice that the contents of the git repository are available for use. You can select a branch, pull the latest files, and, on the upper right hand corner of the screen, you can make commits and push updates back to your repository. \n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000000"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "AGENDA"
   },
   "source": [
    "In this Notebook ([on Container Runtime](https://docs.snowflake.com/developer-guide/snowflake-ml/notebooks-on-spcs)), we will develop a machine learning model that accurately predicts the \"mortgage response\" (e.g., loan approval, offer acceptance) based on borrower characteristics and loan details.\n",
    "\n",
    "We will showcase all the typical steps in a machine learning pipeline using native capabilities in Snowflake through this use case:\n",
    "\n",
    "### 1. `FEATURE ENGINEERING:` Use [Snowflake Feature Store](https://docs.snowflake.com/en/developer-guide/snowflake-ml/feature-store/overview) to track engineered features\n",
    "- Store feature defintions in a feature store for reproducible computation of ML features\n",
    "      \n",
    "### 2. `DISTRIBUTED MODEL TRAINING, DEPLOYMENT, & INFERENCE`\n",
    "- Distributed XGBoost via [Snowflake's Distributed Modeling Classes](https://docs.snowflake.com/en/developer-guide/snowpark-ml/reference/latest/modeling_distributors) - multi-node, multi-GPU\n",
    "- Explore [Snowflake Model Registry](https://docs.snowflake.com/en/developer-guide/snowflake-ml/model-registry/overview) capabilities such as **metadata tracking, model metrics tracking, and inference/deployment**\n",
    "- [Deploy model to Snowpark Container Services](https://docs.snowflake.com/en/developer-guide/snowflake-ml/model-registry/container)\n",
    "\n",
    "### 3. `ML OBSERVABILITY:` Set up a [Model Monitor](https://docs.snowflake.com/en/developer-guide/snowflake-ml/model-registry/model-observability) to track 1 year of predicted and actual loan repayments\n",
    "- Compute performance metrics such a F1, Precision, Recall\n",
    "- Inspect model drift (i.e. how much has the average predicted repayment rate changed day-to-day)\n",
    "- Compare models side-by-side to understand which model should be used in production\n",
    "Identify and understand data issues"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000001"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "BEST_PRACTICE_PIP"
   },
   "source": [
    "# Best practice: Augment our robust container runtimes with pip package management to tailor a python environment to your needs"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000002"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce110000-1111-2222-3333-ffffff000003",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "imports_and_session",
    "resultHeight": 84
   },
   "outputs": [],
   "source": "# Built-in\nimport math\nimport pickle\nfrom datetime import datetime\n\n# Third-party\nimport pandas as pd\nimport numpy as np\nimport shap\nimport sklearn\nimport streamlit as st\nfrom xgboost import XGBClassifier\n\n# Snowflake ML\nfrom snowflake.ml.registry import Registry\nfrom snowflake.ml.feature_store import FeatureStore, FeatureView, Entity, CreationMode\nfrom snowflake.ml.runtime_cluster import scale_cluster, get_nodes\n\n# Snowpark Core\nfrom snowflake.snowpark import DataFrame, Window\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark.functions import (\n    col,\n    to_timestamp,\n    min,\n    max,\n    month,\n    dayofmonth,\n    dayofweek,\n    dayofyear,\n    avg,\n    date_add,\n    sql_expr,\n)\nfrom snowflake.snowpark.types import IntegerType\n\n# Initialize session\nsession = get_active_session()\n\n# Use schema DEFAULT_SCHEMA to create new Snowflake objects in this project\nsession.use_schema('DEFAULT_SCHEMA')\nsession.use_role('E2E_ML_ROLE')\n\nsession\n\nimport logging\n#logging.basicConfig(level=logging.WARN)\nlogging.getLogger().setLevel(logging.WARN)"
  },
  {
   "cell_type": "code",
   "id": "695802e6-b70f-42ef-bbfd-e068a8bfd072",
   "metadata": {
    "language": "python",
    "name": "SET_VERSION"
   },
   "outputs": [],
   "source": "# Let's set a version (must be a string) that will be used through all the artifacts \n# that will be created for our project.\nVERSION_NUM = 'ML303_V1'",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c150a19a-9fc5-48ab-8633-4a306ce3097a",
   "metadata": {
    "collapsed": false,
    "name": "md_read_data"
   },
   "source": [
    "We can read in the .csv file and store the data as a Snowflake table. Then, we can read in the data as a [Snowpark dataframe](https://docs.snowflake.com/en/developer-guide/snowpark/python/working-with-dataframes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e853f0a-321f-4bbd-a69d-44b9b6ad8839",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "read_data"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Reading table data...\")\n",
    "    df = session.table(\"MORTGAGE_LENDING_DEMO_DATA\")\n",
    "    df.show(5)\n",
    "except:\n",
    "    print(\"Table not found! Uploading data to snowflake table\")\n",
    "    df_pandas = pd.read_csv(\"MORTGAGE_LENDING_DEMO_DATA.csv.zip\")\n",
    "    session.write_pandas(df_pandas, \"MORTGAGE_LENDING_DEMO_DATA\", auto_create_table=True)\n",
    "    df = session.table(\"MORTGAGE_LENDING_DEMO_DATA\")\n",
    "    df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "id": "e1e0601e-f6e9-47c2-9dbb-ce163e02b72f",
   "metadata": {
    "language": "python",
    "name": "augment_timestamps"
   },
   "outputs": [],
   "source": "# Update timestamp info using Snowpark APIs\ncurrent_time = datetime.now()\ndf_max_time = datetime.strptime(str(df.select(max(\"TS\")).collect()[0][0]), \"%Y-%m-%d %H:%M:%S.%f\")\n    \ntimedelta = current_time- df_max_time\ndf = df.with_columns(\n        [\"TIMESTAMP\", \"MONTH\", \"DAY_OF_YEAR\", \"DOTW\"],\n        [date_add(to_timestamp(\"TS\"), timedelta.days-1),\n            month(\"TIMESTAMP\"),\n            dayofyear(\"TIMESTAMP\"),\n            dayofweek(\"TIMESTAMP\")])",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8aa46c7d-519b-422c-8932-9b031fc6b4bd",
   "metadata": {
    "collapsed": false,
    "name": "___FEATURE_ENG___"
   },
   "source": [
    "# Feature Engineering with Snowpark APIs\n",
    "\n",
    "We will create a number of features within `create_mortgage_features()`:\n",
    "- Timestamp features (i.e. month, day of year, day of week)\n",
    "- Income and Loan features\n",
    "- County-level income stats\n",
    "- High income flag\n",
    "- Time-based rolling average.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "BEST_PRACTICE_FS_1",
    "collapsed": false
   },
   "source": "# Best Practice: Use the language you are most comfortable with to load and transform your Snowflake data, efficiently\n\nThe features below are created using a combination of the Snowpark Dataframe API and SQL expressions. \nYou can find the Snowpark API reference documentation [here](https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/1.2.0/index). Popular native python libraries like Pandas are always available for use within a Snowflake Notebook, and the Snowpark Pandas API can also be used to scale Pandas-based operational syntax to be applied to Snowflake table data.  ",
   "id": "ce110000-1111-2222-3333-ffffff000004"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40b85cc-3bdb-48f9-b562-c67821d65bed",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "create_features"
   },
   "outputs": [],
   "source": "def create_mortgage_features(df):\n    # Step 2: Income and loan features (Per-row features)\n    df = df.with_columns(\n        [\"LOAN_AMOUNT\", \"LOAN_PURPOSE_NAME\", \"INCOME\", \"INCOME_LOAN_RATIO\"],\n        [\n            col(\"LOAN_AMOUNT_000s\")*1000,\n            col(\"LOAN_PURPOSE_NAME\"),\n            col(\"APPLICANT_INCOME_000s\")*1000,\n            col(\"INCOME\")/col(\"LOAN_AMOUNT\")\n        ]\n    )\n    \n    # Step 3: County-level income stats (Per-group features)\n    county_income_df = df.group_by([\"COUNTY_NAME\"]).agg(\n        avg(\"INCOME\").alias(\"AVG_COUNTY_INCOME\")\n    )\n    # Join back to the original dataframe\n    df = df.join(county_income_df, \"COUNTY_NAME\")\n    \n    # Step 4: Add high income flag\n    df = df.with_column(\n        \"HIGH_INCOME_FLAG\", \n        (col(\"INCOME\") > col(\"AVG_COUNTY_INCOME\")).astype(IntegerType())\n    )\n    \n    # Step 5: Time-based rolling average\n    df = df.with_column(\n        \"AVG_THIRTY_DAY_LOAN_AMOUNT\",\n        sql_expr(\"\"\"\n            AVG(LOAN_AMOUNT) OVER (\n                PARTITION BY COUNTY_NAME \n                ORDER BY TIMESTAMP \n                RANGE BETWEEN INTERVAL '30 DAYS' PRECEDING AND CURRENT ROW\n            )\n        \"\"\")\n    )\n\n    return df"
  },
  {
   "cell_type": "markdown",
   "id": "2723dbb3-2519-4c56-8c36-c9294f2a3190",
   "metadata": {
    "collapsed": false,
    "name": "md_df_explain"
   },
   "source": [
    "We can even see what the actual SQL execution plan in under the hood from defining all the feature logic above using Snowpark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c4ead8-25ac-46cc-9bd9-17eac2f796d5",
   "metadata": {
    "name": "df_explain",
    "collapsed": false
   },
   "source": "feature_df.explain()"
  },
  {
   "cell_type": "markdown",
   "id": "72d7645e-e0ac-4539-b132-54ce53431402",
   "metadata": {
    "collapsed": false,
    "name": "md_fs"
   },
   "source": [
    "## Now, we can create a [Snowflake Feature Store](https://docs.snowflake.com/en/developer-guide/snowflake-ml/feature-store/overview).\n",
    "\n",
    "### How does the Snowflake Feature Store work?\n",
    "A feature store is used to store, retieve, and manage features within **feature views**. \n",
    "\n",
    "A **feature view** encapsulates a Python or SQL pipeline for transforming raw data into one or more related features. Feature views are organized by **entities**. An **entity** represents one or more ID columns that represents a unique row of feature data. \n",
    "\n",
    "Here, the entithy of interest is a loan: we create an entity based on the field LOAN_ID. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abacdc71-9f2c-419f-8d50-3e8f89be367f",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "define_feature_store",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "fs = FeatureStore(\n    session=session, \n    database=session.get_current_database(), \n    name=session.get_current_schema(), \n    default_warehouse=session.get_current_warehouse(),\n    creation_mode=CreationMode.CREATE_IF_NOT_EXIST)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91d6d39-7819-4825-8729-a3f19ca5cdf7",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "load_or_register_entity",
    "resultHeight": 38
   },
   "outputs": [],
   "source": [
    "# First try to retrieve an existing entity definition\n",
    "# If not, define a new one and register\n",
    "try:\n",
    "    # Retrieve existing entity\n",
    "    loan_id_entity = fs.get_entity('LOAN_ENTITY') \n",
    "    print('Retrieved existing entity')\n",
    "except:\n",
    "    # Define new entity\n",
    "    loan_id_entity = Entity(\n",
    "        name = \"LOAN_ENTITY\",\n",
    "        join_keys = [\"LOAN_ID\"],\n",
    "        desc = \"Features defined on a per loan level\")\n",
    "    # Register\n",
    "    fs.register_entity(loan_id_entity)\n",
    "    print(\"Registered new entity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67480d6a-183f-4373-aaa8-d3ed8e80e11d",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "list_entities",
    "resultHeight": 111
   },
   "outputs": [],
   "source": [
    "fs.list_entities()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf84fe3-4120-4092-b43d-8873da57d461",
   "metadata": {
    "collapsed": false,
    "name": "md_feature_view"
   },
   "source": "Now, we can create a [Feature View](https://docs.snowflake.com/en/developer-guide/snowflake-ml/feature-store/feature-views) based on the feature engineering logic we defined in the function **create_mortgage_features** and applied to the data we read into a Snowpark DF."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b53364f-90c4-45b4-94ee-b2fde6f93475",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "feature_view_creation",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "# Define and register feature view\nloan_fv = FeatureView(\n    name=\"Mortgage_Feature_View\",\n    entities=[loan_id_entity],\n    feature_df=create_mortgage_features(df).select(\n        [\"LOAN_ID\", \"TIMESTAMP\", \"MONTH\", \"DAY_OF_YEAR\", \"DOTW\", \n         \"LOAN_AMOUNT\", \"LOAN_PURPOSE_NAME\", \"INCOME\", \"INCOME_LOAN_RATIO\", \n         \"AVG_COUNTY_INCOME\", \"HIGH_INCOME_FLAG\", \n         \"AVG_THIRTY_DAY_LOAN_AMOUNT\"]),\n    timestamp_col=\"TIMESTAMP\",\n    refresh_freq=\"1 day\",\n    refresh_mode=\"INCREMENTAL\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "feature_descriptions",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Add feature level descriptions\n",
    "loan_fv = loan_fv.attach_feature_desc(\n",
    "    {\n",
    "        \"MONTH\": \"Month of loan\",\n",
    "        \"DAY_OF_YEAR\": \"Day of calendar year of loan\",\n",
    "        \"DOTW\": \"Day of the week of loan\",\n",
    "        \"LOAN_AMOUNT\": \"Loan amount in $USD\",\n",
    "        \"INCOME\": \"Household income in $USD\",\n",
    "        \"INCOME_LOAN_RATIO\": \"Ratio of LOAN_AMOUNT/INCOME\",\n",
    "        \"AVG_COUNTY_INCOME\": \"Average household income aggregated at county level\",\n",
    "        \"HIGH_INCOME_FLAG\": \"Binary flag to indicate whether household income is higher than MEDIAN_COUNTY_INCOME\",\n",
    "        \"AVG_THIRTY_DAY_LOAN_AMOUNT\": \"Rolling 30 day average of LOAN_AMOUNT\"\n",
    "    }\n",
    ")"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000006"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "register_fv",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "## Register (add) the feature view to our feature store) \n",
    "loan_fv = fs.register_feature_view(loan_fv, version=VERSION_NUM, overwrite=True)"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000007"
  },
  {
   "cell_type": "markdown",
   "id": "e4d2aecb-3593-41ab-8951-3847e707bdaa",
   "metadata": {
    "collapsed": false,
    "name": "md_list_feature_views"
   },
   "source": [
    "Let's take a look at the existing feature views we have in our Feature Store through the Python API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c3225b-b936-4aa7-81f2-27bbaeee1c0f",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "show_feature_views",
    "resultHeight": 111
   },
   "outputs": [],
   "source": [
    "fs.list_feature_views()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85718b0d-55f0-4013-a4fd-57bdee356322",
   "metadata": {
    "collapsed": false,
    "name": "md_inspect_ui"
   },
   "source": [
    "You can also inspect your feature view in the Feature Store UI link generated in the next cell. \n",
    "\n",
    "You can also click on the `Lineage` tab in the Feature View to look at the lineage of these features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96ff67f-bb04-40cb-8c14-11b5ebb2917d",
   "metadata": {
    "collapsed": false,
    "name": "md_dataset"
   },
   "source": [
    "# Feature Retrieval \n",
    "### Retrieve a [Dataset from the feature view for model training](https://docs.snowflake.com/en/developer-guide/snowflake-ml/feature-store/modeling#generating-snowflake-datasets-for-training)\n",
    "\n",
    "We can now generate a [Snowflake Dataset](https://docs.snowflake.com/en/developer-guide/snowflake-ml/dataset), which are immutable, file-based objects that exist within your Snowpark session. They can be converted to Snowpark Dataframes, or written to persistent Snowflake objects as needed.\n",
    "\n",
    "First, we create a **spine dataframe** which will contain the rows and entity IDs that we want to include in our training data. In this example, we are selecting all the rows in our source data. "
   ]
  },
  {
   "cell_type": "code",
   "id": "f1c9e82c-4288-465f-8e52-b91020b57b9d",
   "metadata": {
    "language": "python",
    "name": "revisit_source_df"
   },
   "outputs": [],
   "source": "df.limit(5)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1687ccf1-cab1-48b5-a914-f6665cf35a77",
   "metadata": {
    "language": "python",
    "name": "generate_spine_df",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Create a spine dataframe to select the desired rows for training. \n# In this case we are using all the rows \nspine_df = df.select(\"LOAN_ID\",\"TIMESTAMP\", \"MORTGAGERESPONSE\") #only need the features used to fetch rest of feature view\nspine_df.show()"
  },
  {
   "cell_type": "markdown",
   "id": "a2f06788-4a31-4357-83af-e1c1d8bee194",
   "metadata": {
    "collapsed": false,
    "name": "md_generate_dataset"
   },
   "source": "The `generate_dataset()` API will fill this spine_df with the correct feature values from a feature view that has been registered in the feature store. \n\nHere, we already have the feature view in memory, but we demonstrate how to load a feature view from the feature store, for reference. "
  },
  {
   "cell_type": "code",
   "id": "de64c2be-d97f-4f0c-ab86-dd35f7a603a0",
   "metadata": {
    "language": "python",
    "name": "retrieve_fv",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "mortgage_feature_view = fs.get_feature_view(\n    name=\"MORTGAGE_FEATURE_VIEW\",\n    version=\"ML303_V1\"\n)\n\nmortgage_feature_view",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535efc80-e4fc-41c5-98eb-5b5450bcf199",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "generate_dataset",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "ds = fs.generate_dataset(\n    name=f\"MORTGAGE_DATASET_EXTENDED_FEATURES_{VERSION_NUM}\",\n    # only need the features used to fetch rest of feature view\n    spine_df=df.select(\"LOAN_ID\", \"TIMESTAMP\", \"MORTGAGERESPONSE\"), \n    features=[mortgage_feature_view],\n    spine_timestamp_col=\"TIMESTAMP\",\n    spine_label_cols=[\"MORTGAGERESPONSE\"]\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdaa537-3fb9-476c-9153-3236edfdfcb3",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "convert_dataset_to_snowpark_and_pandas",
    "resultHeight": 239
   },
   "outputs": [],
   "source": [
    "ds_sp = ds.read.to_snowpark_dataframe()\n",
    "ds_sp.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68269d55-c869-41a2-9329-8c47178bba17",
   "metadata": {
    "collapsed": false,
    "name": "BEST_PRACTICE_RUNTIME"
   },
   "source": [
    "# Best practice: Choose the runtime that best suits each step, reduce configuration effort and operational burden\n",
    "\n",
    "Here, we will use [Snowflake ML distributed preprocessors such as OneHotEncoder](https://docs.snowflake.com/en/developer-guide/snowflake-ml/modeling#preprocessing) which are implementations of sklearn preprocessors and run in parallel on a Snowflake **Warehouse**. \n",
    "\n",
    "Alternatively, one could use OSS sklearn preprocessors directly on the **Container Runtime** node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e17036-7a69-4915-b025-49c900aeb46b",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "one_hot_encoding",
    "resultHeight": 360
   },
   "outputs": [],
   "source": [
    "import snowflake.ml.modeling.preprocessing as snowml\n",
    "from snowflake.snowpark.types import StringType\n",
    "\n",
    "OHE_COLS = ds_sp.select([col.name for col in ds_sp.schema if col.datatype==StringType()]).columns\n",
    "OHE_POST_COLS = [i+\"_OHE\" for i in OHE_COLS]\n",
    "\n",
    "# Encode categoricals to numeric columns\n",
    "snowml_ohe = snowml.OneHotEncoder(input_cols=OHE_COLS, output_cols = OHE_COLS, drop_input_cols=True)\n",
    "ds_sp_ohe = snowml_ohe.fit(ds_sp).transform(ds_sp)\n",
    "\n",
    "# Rename columns to avoid double nested quotes and white space chars\n",
    "rename_dict = {}\n",
    "for i in ds_sp_ohe.columns:\n",
    "    if '\"' in i:\n",
    "        rename_dict[i] = i.replace('\"','').replace(' ', '_')\n",
    "\n",
    "ds_sp_ohe = ds_sp_ohe.rename(rename_dict)\n",
    "ds_sp_ohe.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb14f23-2537-4c0f-85e8-c97368879d9e",
   "metadata": {
    "collapsed": false,
    "name": "md_split_sets"
   },
   "source": [
    "Now, we're ready to split the processed data into train/test sets and fill any null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d834f6f3-ce15-405e-8fec-1d1bb5c224a6",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "train_test_split",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "train, test = ds_sp_ohe.random_split(weights=[0.70, 0.30], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ff103e-5314-4e95-87ba-d784b1102f36",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "fill_na",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "cols = list(ds_sp_ohe.columns)\n",
    "cols.remove(\"TIMESTAMP\")\n",
    "\n",
    "train = train.fillna(0, subset=cols)\n",
    "test = test.fillna(0, subset=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f44c10-7b1d-48f1-95f4-ffadb23b0a4e",
   "metadata": {
    "collapsed": false,
    "name": "___TRAIN_DEPLOY___"
   },
   "source": [
    "# Distributed model training & deployment\n",
    "\n",
    "Now we demonstrate how to execute distributed model training using Snowflake's [distributed modeling classes](https://docs.snowflake.com/en/developer-guide/snowpark-ml/reference/latest/modeling_distributors).\n",
    "\n",
    "Snowflake will set up a ray cluster on all available nodes in your compute pool (CPU or GPU) and execute the distributed training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bb1b94-1cfd-4c5e-8a62-3e3b4b1b9c07",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "_ray_resource"
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "import pprint\n",
    "\n",
    "def _format_resources(resources):\n",
    "    \"\"\"Convert memory fields to GB and filter out internal node tags.\"\"\"\n",
    "    formatted = {}\n",
    "    for k, v in resources.items():\n",
    "        # Skip internal node identifiers\n",
    "        if k.startswith(\"node:\"):\n",
    "            continue\n",
    "        if k in {\"memory\", \"object_store_memory\"}:\n",
    "            formatted[k] = f\"{v / (1024 ** 3):.2f} GB\"\n",
    "        else:\n",
    "            formatted[k] = v\n",
    "    return formatted\n",
    "\n",
    "def show_ray_cluster_resources():\n",
    "    \"\"\"Nicely formatted cluster-wide and node-level resource info from Ray.\"\"\"\n",
    "    print(\" Cluster Resources:\")\n",
    "    cluster = _format_resources(ray.cluster_resources())\n",
    "    pprint.pprint(cluster, sort_dicts=True, width=100)\n",
    "\n",
    "    print(\"\\n Node-Level Resources:\")\n",
    "    for node in ray.nodes():\n",
    "        print(f\"\\nNode: {node['NodeManagerAddress']}\")\n",
    "        node_resources = _format_resources(node[\"Resources\"])\n",
    "        pprint.pprint(node_resources, sort_dicts=True, width=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbf3e6b-cf17-446b-866d-f98dabc49c1b",
   "metadata": {
    "collapsed": false,
    "name": "md_scale_up"
   },
   "source": [
    "Let's scale our resources to use all 4 nodes in our cluster using Snowflake ML's `scale_cluster` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31149b3-eaa1-4828-9096-e755fff4b932",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "scale_up_cluster_pretrain"
   },
   "outputs": [],
   "source": "# Scale to max compute pool nodes\nnum_nodes = 4\n\n# Suppress SIGTERM ray warnings \nlogging.basicConfig(level=logging.ERROR)\n\n# Use full path name\nnb_name = \"E2E_ML_NOTEBOOK_DIST\"\nscale_cluster(expected_cluster_size=num_nodes,\n              is_async=True,\n              notebook_name=nb_name)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cfc050-c294-4843-ba42-d3e784ce1644",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "confirm_scale_up"
   },
   "outputs": [],
   "source": [
    "# Show number of nodes after changing cluster manager settings\n",
    "show_ray_cluster_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc34f4fd-080f-4fdb-9039-9da1b7936298",
   "metadata": {
    "name": "option_alter_pool",
    "collapsed": false
   },
   "source": "alter compute pool CP_GPU_NV_S_4 set min_nodes=4;\ndescribe compute pool CP_GPU_NV_S_4;"
  },
  {
   "cell_type": "markdown",
   "id": "5b639f32-72d7-4cad-9ae4-a428a3a7ea87",
   "metadata": {
    "name": "option_alter_suspend",
    "collapsed": false
   },
   "source": " alter compute pool CP_GPU_NV_S_4 set AUTO_SUSPEND_SECS = 3600;\n"
  },
  {
   "cell_type": "markdown",
   "id": "c74fdb36-84bd-46fb-a9c0-5599b7ae61cb",
   "metadata": {
    "collapsed": false,
    "name": "md_ready_to_train"
   },
   "source": [
    "Now, we're ready to do multi-node, multi-GPU training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c59691b-d866-452b-955b-1ef03ea42df6",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "quiet_ray_pretrain"
   },
   "outputs": [],
   "source": [
    "# Quiet messages from Ray\n",
    "context = ray.data.DataContext().get_current() \n",
    "context.execution_options.verbose_progress = False\n",
    "# context.enable_operator_progress_bars = False\n",
    "# context.enable_progress_bars = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "BEST_PRACTICE_DATA_CONN"
   },
   "source": [
    "# Best practice: Data Connectors. \n",
    "## Use Snowflake DataConnector to merge the efficiencies of Snowflake Table structures with distributed ray operation.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000010"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44106da8-2fff-4494-94d4-7122f3007895",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "multi_node_multi_gpu"
   },
   "outputs": [],
   "source": "from snowflake.ml.modeling.distributors.xgboost.xgboost_estimator import XGBEstimator, XGBScalingConfig\nfrom snowflake.ml.data.data_connector import DataConnector\n\n# Use Snowflake's DataConnector to efficiently connect Snowflake data to Ray\ndc = DataConnector.from_dataframe(train)\n\n# Set up the scaling configuration for multi-node and multi-GPU usage\nscaling_config = XGBScalingConfig(\n    num_workers=-1,            # Use all available workers\n    num_cpu_per_worker=-1,     # Use all available CPU cores per worker\n    use_gpu=True               # Enable GPU for training, resources_per_worker={\"GPU\": 1}\n)\n\n# Define XGBoost hyperparameters for GPU\nhyp_params = {\n    \"booster\": \"gbtree\",               # Standard boosting model\n    \"tree_method\": \"gpu_hist\",         # Enables NV_GPU Histogram\n    \"predictor\": \"auto\",               # Uses GPU for prediction\n    \"n_estimators\": 100,               # Number of trees to train\n    \"max_depth\": 5,                    # Maximum tree depth\n    \"grow_policy\": \"lossguide\",        # Enables better GPU scalability by growing trees leaf-wise (instead of depth-wise)\n    \"max_leaves\": 512,                 # Limits tree complexity; balances model capacity and memory usage when using 'lossguide'\n    \"learning_rate\": 0.1,              # Step size for each tree\n    \"subsample\": 0.8,                  # Fraction of samples used for each tree\n    \"colsample_bytree\": 0.8            # Fraction of features used for each tree\n}\n\n# Define distributed xgb estimator\nmulti_node_gpu_xgb = XGBEstimator(\n    params = hyp_params,\n    scaling_config = scaling_config)\n\nmulti_node_gpu_xgb.fit(dc,\n                 input_cols = train.drop([\"TIMESTAMP\", \"LOAN_ID\", \"MORTGAGERESPONSE\"]).columns,\n                 label_col = \"MORTGAGERESPONSE\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "BEST_PRACTICE_MR"
   },
   "source": [
    "# Best practices: Register all models. \n",
    "## Log all your model versions for 'set and forget' experiment tracking. Use Snowpark Dataframes to track data lineage all the way back to the source table, through Feature Engineering.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000011"
  },
  {
   "cell_type": "markdown",
   "id": "93ef9ff0-e9ab-40ca-97bc-b59e7450c369",
   "metadata": {
    "collapsed": false,
    "name": "md_dist_model"
   },
   "source": [
    "### Now, we can log this distributed xgb model to Snowflake Model Registry:\n",
    "- Log models with important metadata\n",
    "- Manage model lifecycles\n",
    "- Serve models from Snowflake runtimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf0526c-34e1-4723-a173-60116bd0e8d6",
   "metadata": {
    "language": "python",
    "name": "model_reg"
   },
   "outputs": [],
   "source": [
    "#Create a snowflake model registry object \n",
    "from snowflake.ml.registry import Registry\n",
    "from snowflake.ml._internal.utils import identifier\n",
    "from snowflake.ml.model import model_signature\n",
    "\n",
    "db = identifier._get_unescaped_name(session.get_current_database())\n",
    "schema = identifier._get_unescaped_name(session.get_current_schema())\n",
    "\n",
    "# Define model name\n",
    "model_name = f\"MORTGAGE_LENDING_MLOPS_{VERSION_NUM}\"\n",
    "\n",
    "# Create a registry to log the model to\n",
    "model_registry = Registry(session=session, \n",
    "                          database_name=db, \n",
    "                          schema_name=schema,\n",
    "                          options={\"enable_monitoring\": True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad284d2-00bc-419f-871b-cfa615e8ab60",
   "metadata": {
    "name": "drop_model_if_needed",
    "collapsed": false
   },
   "source": [
    "DROP MODEL IF EXISTS {{model_name}};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "BEST_PRACTICE_INF_RUNTIME",
    "collapsed": false
   },
   "source": [
    "# Best Practice: Set your runtime for inference with a single parameter. \n",
    "## Use SPCS Model Serving for GPU-acceleration or low-latency requirements."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000012"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479a3b58-42e4-4883-aac2-d680b5830a16",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "log_multi_node_xgb"
   },
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.WARN)\n",
    "\n",
    "# Extract xgb booster object from Snowflake optimized XGB model\n",
    "gpu_booster = multi_node_gpu_xgb.get_booster()\n",
    "\n",
    "# Log the distributed model to the model registry\n",
    "dist_version_name = f'XGB_MULTI_NODE_GPU_DIST'\n",
    "\n",
    "try:\n",
    "    mv_base = model_registry.get_model(model_name).version(dist_version_name)\n",
    "    print(\"Found existing model version!\")\n",
    "except:\n",
    "    print(\"Logging new model version...\")\n",
    "    mv_base = model_registry.log_model(\n",
    "        model_name=model_name,\n",
    "        model=gpu_booster,\n",
    "        version_name=dist_version_name,\n",
    "        sample_input_data = train.drop([\"TIMESTAMP\", \"LOAN_ID\", \"MORTGAGERESPONSE\"]).limit(100), #using snowpark df to maintain lineage\n",
    "        comment = \"\"\"Distributed ML model for predicting loan approval likelihood.\"\"\",\n",
    "        options={'relax_version': True},\n",
    "        target_platforms={\"WAREHOUSE\", \"SNOWPARK_CONTAINER_SERVICES\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a208883-0e48-4685-ac30-0e23c3cbfab6",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "show_versions"
   },
   "outputs": [],
   "source": [
    "SHOW VERSIONS IN MODEL {{model_name}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df060c3-a45b-4999-b229-18cfdeec492c",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "show_models_2"
   },
   "outputs": [],
   "source": [
    "model_registry.show_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3934427e-390d-4f5d-b341-756afa53f8c2",
   "metadata": {
    "collapsed": false,
    "name": "md_scale_down"
   },
   "source": [
    "Since we don't need all nodes anymore, let's scale down our cluster now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1f3de4-685f-402f-8caa-427cdb56938b",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "scale_down_post_train"
   },
   "outputs": [],
   "source": [
    "# Scale down \n",
    "num_nodes = 1\n",
    "\n",
    "# Suppress SIGTERM ray warnings \n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# Use full path\n",
    "nb_name = \"E2E_ML_NOTEBOOK_DIST\"\n",
    "scale_cluster(expected_cluster_size=num_nodes,\n",
    "              # is_async=True,\n",
    "              notebook_name=nb_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c96e53-480c-4df4-8375-e0ad5faec722",
   "metadata": {
    "collapsed": false,
    "name": "md_confirm_scale_down"
   },
   "source": [
    "We can see that 3 of the nodes are empty now, which tells us they're no longer being considered as available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fee6a9-9265-4e2d-848d-f209ba15c891",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "confirm_scale_down"
   },
   "outputs": [],
   "source": [
    "# Show number of nodes after changing cluster manager settings\n",
    "show_ray_cluster_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "BEST_PRACTICE_SERVING"
   },
   "source": [
    "# Best practice: Model Serving via Snowflake Model Registry. \n",
    "## Logging models in the model registry simplifies model serving actions and makes scaling an afterthought"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000013"
  },
  {
   "cell_type": "markdown",
   "id": "64a8deb6-fdc0-464d-9ad2-b41650a2217a",
   "metadata": {
    "collapsed": false,
    "name": "model_deploy_spcs"
   },
   "source": [
    "## Model Deployment to Snowpark Container Services (SPCS)\n",
    "\n",
    "Here, we also show how to deploy a model to Snowpark Container Services as a long running service using [Model Serving](https://docs.snowflake.com/en/developer-guide/snowflake-ml/model-registry/container).\n",
    "\n",
    "This is useful to run GPU based inference, run very large models that do not fit in the Warehouse memory, or when you have additional package dependencies that are not met in the Warehouse. You can also create REST API endpoints for running inference using external HTTP requests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15be76d-6afe-4020-950c-0e8cc3955a66",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "define_deploy_vars"
   },
   "outputs": [],
   "source": [
    "# Define the variables we'll use to create the deployment service.\n",
    "image_repo_name = \"my_inference_images\"\n",
    "cp_name = \"CP_GPU_NV_S_4\"\n",
    "num_spcs_nodes = '2'\n",
    "service_name = 'MORTGAGE_LENDING_PREDICTION_SERVICE'\n",
    "\n",
    "current_database = session.get_current_database().replace('\"', '')\n",
    "current_schema = session.get_current_schema().replace('\"', '')\n",
    "extended_image_repo_name = f\"{current_database}.DEFAULT_SCHEMA.{image_repo_name}\"\n",
    "extended_service_name = f'{current_database}.DEFAULT_SCHEMA.{service_name}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf88db88-6d30-4ca5-aedb-700ab5609c8a",
   "metadata": {
    "name": "drop_service_if_needed",
    "collapsed": false
   },
   "source": [
    "DROP SERVICE IF EXISTS {{service_name}};"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca8088c-7a3e-4d65-b669-dae3eafc907d",
   "metadata": {
    "collapsed": false,
    "name": "md_create_service"
   },
   "source": [
    "Note, we're creating a service based on the model we just logged to the Model Registry: `mv_base`\n",
    "\n",
    "You can also specify pip_requirements if your model has pip dependencies. Here we are selecting `ingress_enabled = True` to also create a REST API endpoint for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96365088-1841-4505-88aa-c4bd472a63ce",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "create_service"
   },
   "outputs": [],
   "source": [
    "mv_base.create_service(\n",
    "    service_name=extended_service_name,\n",
    "    service_compute_pool=cp_name,\n",
    "    image_repo=extended_image_repo_name,\n",
    "    ingress_enabled=True,\n",
    "    max_instances=int(num_spcs_nodes),\n",
    "    build_external_access_integration=\"ALLOW_ALL_INTEGRATION\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04424bbb-33af-4c91-a3d7-e6294c9b6d78",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "show_services"
   },
   "outputs": [],
   "source": [
    "-- Check that the service is created\n",
    "SHOW SERVICES;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc147c6-e180-49f4-a863-f7cfe19a5668",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "get_model_services"
   },
   "outputs": [],
   "source": [
    "mv_base = model_registry.get_model(f\"MORTGAGE_LENDING_MLOPS_{VERSION_NUM}\").version(\"XGB_MULTI_NODE_GPU_DIST\")\n",
    "mv_base.list_services()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ceac92c-b931-4513-ac69-0ba029ed743d",
   "metadata": {
    "collapsed": false,
    "name": "md_run_inference"
   },
   "source": [
    "Now, we can run predictions on test data using this deployed SPCS Model Service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbf6cd1-67e1-4638-95f8-dae3fe69dd4a",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "run_inference"
   },
   "outputs": [],
   "source": [
    "mv_base.run(test, \n",
    "            function_name = \"PREDICT\", \n",
    "            service_name = \"DEFAULT_SCHEMA.MORTGAGE_LENDING_PREDICTION_SERVICE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8209d9-a272-4252-ad0a-bf497031f363",
   "metadata": {
    "collapsed": false,
    "name": "__MODEL_OBS__"
   },
   "source": [
    "# Model Observability (Model Monitoring)\n",
    "[ML Observability](https://docs.snowflake.com/en/developer-guide/snowflake-ml/model-registry/model-observability) allows you to track the quality of production models you have deployed via the Snowflake Model Registry across multiple dimensions, such as performance, drift, and volume.\n",
    "\n",
    "Let's first save our train and test data as Snowflake tables and create a new stage for the next few steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b67dfcd-3778-4133-be27-06bb9523e7f0",
   "metadata": {
    "language": "python",
    "name": "save_train_test"
   },
   "outputs": [],
   "source": [
    "train.write.save_as_table(f\"DEMO_MORTGAGE_LENDING_TRAIN_{VERSION_NUM}\", mode=\"overwrite\")\n",
    "test.write.save_as_table(f\"DEMO_MORTGAGE_LENDING_TEST_{VERSION_NUM}\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b108ce5-5e7b-426a-ab64-578569b6a3e7",
   "metadata": {
    "language": "python",
    "name": "create_stage"
   },
   "outputs": [],
   "source": [
    "session.sql(\"CREATE STAGE IF NOT EXISTS ML_STAGE\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33474d3e-12a8-4b55-8c6a-d8c669bfe81b",
   "metadata": {
    "collapsed": false,
    "name": "md_inference_sproc"
   },
   "source": [
    "We can create stored procedures for running model inference. These procedures can be scheduled or orchestrated to run continuous inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867671fe-fe59-455b-9f75-a2662232eb53",
   "metadata": {
    "language": "python",
    "name": "inference_sproc"
   },
   "outputs": [],
   "source": [
    "from snowflake import snowpark\n",
    "from snowflake.ml.registry import Registry\n",
    "import joblib\n",
    "import os\n",
    "import logging\n",
    "from snowflake.ml.modeling.pipeline import Pipeline\n",
    "import snowflake.ml.modeling.preprocessing as pp\n",
    "from snowflake.snowpark.types import StringType, IntegerType\n",
    "import snowflake.snowpark.functions as F\n",
    "\n",
    "def inference_sproc(session: snowpark.Session, table_name: str, modelname: str, modelversion: str) -> str:\n",
    "    database=session.get_current_database()\n",
    "    schema=session.get_current_schema()\n",
    "    reg = Registry(session=session)\n",
    "    m = reg.get_model(model_name)  # Fetch the model using the registry\n",
    "    mv = m.version(modelversion)\n",
    "    \n",
    "    input_table_name=table_name\n",
    "    pred_col = f'{modelversion}_PREDICTION'\n",
    "\n",
    "    # Read the input table to a dataframe\n",
    "    df = session.table(input_table_name)\n",
    "    # 'results' is the output DataFrame with predictions\n",
    "    results = mv.run(df, \n",
    "                     function_name=\"predict\",\n",
    "                     service_name=\"DEFAULT_SCHEMA.MORTGAGE_LENDING_PREDICTION_SERVICE\").select(\"LOAN_ID\",'\"output_feature_0\"').withColumnRenamed('\"output_feature_0\"', pred_col)\n",
    "\n",
    "    final = df.join(results, on=\"LOAN_ID\", how=\"full\")\n",
    "    final = final.with_column(pred_col,\n",
    "                              F.round(col(pred_col)))\n",
    "    \n",
    "    # Write results back to Snowflake table\n",
    "    final.write.save_as_table(table_name, mode='overwrite', enable_schema_evolution=True)\n",
    "\n",
    "    return \"Success\"\n",
    "\n",
    "# Register the stored procedure\n",
    "session.sproc.register(\n",
    "    func=inference_sproc,\n",
    "    name=\"model_inference_sproc\",\n",
    "    replace=True,\n",
    "    is_permanent=True,\n",
    "    stage_location=\"@ML_STAGE\",\n",
    "    packages=['joblib', 'snowflake-snowpark-python', 'snowflake-ml-python'],\n",
    "    return_type=StringType()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bd1388-fdd0-46c8-b2b2-efb711deaab1",
   "metadata": {
    "collapsed": false,
    "name": "md_call_sproc"
   },
   "source": [
    "## Inference Execution via stored procedure\n",
    "We'll call our stored procedure 2 times which will execute inference using the deployed model on the train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2033373-60a5-4f5c-88dc-d8d2d72a436b",
   "metadata": {
    "language": "sql",
    "name": "call_sproc_train"
   },
   "outputs": [],
   "source": [
    "CALL model_inference_sproc(\n",
    "    'DEMO_MORTGAGE_LENDING_TRAIN_{{VERSION_NUM}}',\n",
    "    '{{model_name}}', \n",
    "    '{{dist_version_name}}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c420eee-fb1b-4fd2-ad52-30dded9b060e",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "call_sproc_test"
   },
   "outputs": [],
   "source": [
    "CALL model_inference_sproc(\n",
    "    'DEMO_MORTGAGE_LENDING_TEST_{{VERSION_NUM}}',\n",
    "    '{{model_name}}', \n",
    "    '{{dist_version_name}}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aacc1fc-2193-4ce0-9aab-9b9401fcb224",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "view_preds"
   },
   "outputs": [],
   "source": [
    "SELECT * FROM DEMO_MORTGAGE_LENDING_TEST_{{VERSION_NUM}} LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "BEST_PRACTICE_ML_OBS"
   },
   "source": [
    "# Best practice: Use ML Observability to monitor models in in production without making copies of data or maintaining separate infrastructure.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000014"
  },
  {
   "cell_type": "markdown",
   "id": "2b9905ac-a793-4d93-ac45-ed912a70b44b",
   "metadata": {
    "collapsed": false,
    "name": "md_model_monitor"
   },
   "source": [
    "# Create Model Monitor\n",
    "\n",
    "We'll now create a model monitor for the model and compute the prediction drift.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db322e2-091f-495c-9602-e320d22fc0b4",
   "metadata": {
    "language": "sql",
    "name": "create_monitor"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE MODEL MONITOR MORTGAGE_LENDING_DIST_MODEL_MONITOR\n",
    "WITH\n",
    "    MODEL={{model_name}}\n",
    "    VERSION={{dist_version_name}}\n",
    "    FUNCTION=predict\n",
    "    SOURCE=DEMO_MORTGAGE_LENDING_TEST_{{VERSION_NUM}}\n",
    "    BASELINE=DEMO_MORTGAGE_LENDING_TRAIN_{{VERSION_NUM}}\n",
    "    TIMESTAMP_COLUMN='TIMESTAMP'\n",
    "    PREDICTION_SCORE_COLUMNS=('XGB_MULTI_NODE_GPU_DIST_PREDICTION')  \n",
    "    ACTUAL_SCORE_COLUMNS=('MORTGAGERESPONSE')\n",
    "    ID_COLUMNS=('LOAN_ID')\n",
    "    WAREHOUSE={{session.get_current_warehouse()}}\n",
    "    REFRESH_INTERVAL='1 min'\n",
    "    AGGREGATION_WINDOW='1 day';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db14e765-e195-4598-8a5a-9e933760a300",
   "metadata": {
    "language": "sql",
    "name": "model_drift",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": [
    "SELECT * FROM TABLE(MODEL_MONITOR_DRIFT_METRIC(\n",
    "    'MORTGAGE_LENDING_DIST_MODEL_MONITOR', 'DIFFERENCE_OF_MEANS', \n",
    "    'XGB_MULTI_NODE_GPU_DIST_PREDICTION', \n",
    "    '1 DAY', \n",
    "    TO_TIMESTAMP_TZ('2024-06-01'), \n",
    "    TO_TIMESTAMP_TZ('2024-09-01')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dd35ac-cbc5-41c0-95ee-caa59aa19894",
   "metadata": {
    "collapsed": false,
    "name": "md_drop_service"
   },
   "source": [
    "Since we created a REST API above, this service will run continuously. It is a good idea to drop or suspend the service if you do not need it. Compute pool will automatically suspend if no service is running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952e6063-6d6e-44d3-9b68-e87ca9d5134e",
   "metadata": {
    "name": "drop_service"
   },
   "source": [
    "-- DROP SERVICE IF EXISTS {{service_name}};\n",
    "-- SHOW SERVICES;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0119d044-db53-48da-a915-1f775f86f8a1",
   "metadata": {
    "name": "drop_model"
   },
   "source": [
    "-- DROP MODEL IF EXISTS {{model_name}};\n",
    "-- SHOW MODELS;"
   ]
  }
 ]
}